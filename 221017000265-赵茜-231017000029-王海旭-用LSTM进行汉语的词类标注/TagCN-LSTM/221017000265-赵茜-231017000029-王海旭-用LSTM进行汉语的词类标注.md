## 团队成员
   * 赵茜   221017000265  数据预处理，代码编写，模型训练
   * 王海旭 231017000029  主题选择，文档撰写，测试

<br>   


# 课题：用LSTM进行汉语的词类标注

## 介绍
LSTM（长短期记忆网络）是一种循环神经网络（RNN）的变体，被广泛用于自然语言处理（NLP）任务中，包括词性标注（词类分析）。

在汉语词类分析中，LSTM可以用于序列标注，将输入的汉语句子中的每个词汇标注为其相应的词类或词性。下面是使用LSTM进行汉语词类分析的主要步骤：

1. **数据预处理**：收集并准备训练数据集，每个句子应该包括词汇和对应的词类标签。对句子进行分词和词性标注，并将其转换成模型可以处理的格式。

2. **构建模型**：使用深度学习框架（如TensorFlow、PyTorch等）创建LSTM模型。模型通常包括嵌入层（将词汇映射到向量表示）、LSTM层（用于捕捉词汇间的上下文信息）、全连接层和输出层。

3. **训练模型**：利用准备好的数据集对模型进行训练。通过传递句子的词汇序列给模型，并根据每个词汇的词类标签进行监督学习，调整模型参数以提高预测准确度。

4. **评估模型**：使用独立的验证集或测试集评估模型的性能。通常采用准确率、精确度、召回率和F1值等指标来衡量模型在词类分析任务上的表现。

5. **应用模型**：在模型经过验证并具有良好性能后，可以将其应用于新的、未见过的汉语句子，进行词类分析并预测每个词汇的词类。

需要注意的是，对于汉语词类分析，有些词类之间可能存在歧义，需要模型能够处理上下文信息以更准确地预测词类。此外，合适的数据预处理、模型选择和调整参数也对模型性能至关重要。

<br> 

## 环境需求

1.  PyTorch 2.1.1，pytorch-cuda 11.8
2.  Python 3.11
3.  Numpy, sklearn, matplotlib, seaborn
4.  我们的显卡能力
```c
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 529.08       Driver Version: 529.08       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   56C    P0    N/A /  N/A |      0MiB /  2048MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```


## 创建并激活Anaconda虚拟环境

1. 创建pytorch虚拟环境
```
conda create -n pytorch python=3.11 #
```
2. 安装pytorch
```
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 
```

3. 安装其他python包
```
conda install numpy
conda install scikit-learn
conda install matplotlib
conda install seaborn
```

<br> 

## 代码说明
1. 数据获取与准备 dataset.py
    ```python 
    import numpy as np
    import torch

    # 超参数
    class Config:
        TAG_to_ix = {"n":0,"t":1,"s":2,"f":3,"m":4,"q":5,"b":6,"r":7,"v":8,"a":9,"z":10,"d":11,"p":12,"c":13,\
                "u":14,"y":15,"e":16,"o":17,"i":18,"l":19,"j":20,"h":21,"k":22,"g":23,"x":24,"w":25} # 空的补0
        TAG_to_ix_crf = {"n":0,"t":1,"s":2,"f":3,"m":4,"q":5,"b":6,"r":7,"v":8,"a":9,"z":10,"d":11,"p":12,"c":13,\
                "u":14,"y":15,"e":16,"o":17,"i":18,"l":19,"j":20,"h":21,"k":22,"g":23,"x":24,"w":25,"<START>":26,"<STOP>":27}

        FILE_NAME = "./data/1998-01-2003_shuf.txt"

        EMBEDDING_DIM = 100#6 # 词向量的特征维度
        HIDDEN_DIM = 200 # 隐藏层的特征维度
        LAYER = 1 #LSTM层数
        MAX_len = 660

        DROP_RATE = 0.5 #Dropout概率
        BATCH_SIZE = 24 # 批大小
    
        LR = 0.015
        weight_decay = 1.0e-8
        gamma = 0.98
        step = 200
    

    config = Config()

    #人民日报语料
    class Dataset(object):
        def __init__(self, config=config):
            self.config = config
            self.TAG_list = []
            self.TAG_list_test = []
            for i in range(len(self.config.TAG_to_ix)):
                self.TAG_list.append(0)
                self.TAG_list_test.append(0)

            self.batch_size = self.config.BATCH_SIZE
            self.training_data, self.tag_weights = self.get_training_set()
            self.testing_data = self.get_testing_set()
            self.word_to_ix = self.get_word_to_ix()
            self.word_to_ix_length = len(self.word_to_ix) #词表长度

        def get_training_set(self): 
            '''
            return training data: list[ word_seq, tag_seq ] 原始数据，未经过编码
                   tag_weights: 各tag类别权重
            '''
            training_data = []
            print(self.config.FILE_NAME)
            self.lines = open(self.config.FILE_NAME).readlines()
            for line in self.lines[:int(len(self.lines)*0.8)]:
                if not len(line.strip()): # 跳过空行
                    continue
                word_str = ' '
                tag_list = ' '
                for item in line.strip().split()[1:]:
                    if item.split('/')[0][0]=='[':
                        word_str += item.split('/')[0][1:]+' '
                    else:
                        word_str += item.split('/')[0]+' '
                    if item.split('/')[1][0].isupper(): #大写
                        tag_now = item.split('/')[1][1]
                        tag_list += tag_now +' '
                    else:
                        tag_now = item.split('/')[1][0]
                        tag_list += tag_now+' '

                    list_keys= [ i for i in self.config.TAG_to_ix.keys()]
                    self.TAG_list[list_keys.index(tag_now)] += 1

                training_data.append((word_str.split(), tag_list.split()))

            tag_numpy = np.array(self.TAG_list,dtype=np.float32)
            tag_weights = (tag_numpy+1) / np.sum(tag_numpy) # +1是为防止某一类没有数据
            tag_weights = np.median(tag_weights)/tag_weights

            return training_data,tag_weights

        def get_testing_set(self):
            '''
            return testing data: list[ word_seq, tag_seq ] 原始数据，未经过编码
            '''
            testing_data = []
            self.lines = open(self.config.FILE_NAME).readlines()
            for line in self.lines[int(len(self.lines)*0.8):]:
                if not len(line.strip()):
                    continue
                word_str = ' '
                tag_list = ' '
                for item in line.strip().split()[1:]:
                    if item.split('/')[0][0]=='[':
                       word_str += item.split('/')[0][1:]+' '
                    else:
                        word_str += item.split('/')[0]+' '

                    if item.split('/')[1][0].isupper(): #大写
                        tag_now = item.split('/')[1][1]
                        tag_list += tag_now +' '
                    else:
                        tag_now = item.split('/')[1][0]
                        tag_list += tag_now+' '
                    list_keys= [ i for i in self.config.TAG_to_ix.keys()]
                    self.TAG_list_test[list_keys.index(tag_now)] += 1

                testing_data.append((word_str.split(), tag_list.split()))

            return testing_data

        def get_word_to_ix(self):
            '''
            return self.word_to_ix: 为所有词赋予编号，词表
            '''
            word_to_ix = {}
            for sent,tag in self.training_data + self.testing_data:
                for word in sent:
                    if word not in word_to_ix:#防止重复记录单词
                        word_to_ix[word] = len(word_to_ix)+1 #空的情况是补0
            #print('word table length:',len(word_to_ix))
            return word_to_ix # 56182 

        def prepare_sequence(self, seq, to_ix): 
            '''
            # 通过查询词表to_ix, 返回seq中每个词的索引，返回list形式
            '''
            idxs = [to_ix[w] for w in seq]
            return idxs

        def gene_batch(self, data, _iter):
            '''
            traindata loader for training;
            对不同长度的句子进行补齐，便于batch-based train
            '''
            batch_i = 0  #句子长度不同！！！
            train_batch_sentences = []
            train_batch_tags = []
            train_len = []
            for sentence, tag in data[_iter:]:
                train_len.append(len(sentence))

                pad_sentence = self.prepare_sequence(sentence, self.word_to_ix)
                pad_tag = self.prepare_sequence(tag, self.config.TAG_to_ix)

                # padding word_seq to the SAME len
                while len(pad_sentence)!=self.config.MAX_len:
                    pad_sentence.append(0)

                # get a batch
                train_batch_sentences.append(pad_sentence)
                train_batch_tags.append(pad_tag)

                batch_i += 1
                if batch_i == self.batch_size:
                    break   

            # 对batch中所有word_seq按长度降序排列
            down_index = np.array(train_len).argsort()
            down_index_list = down_index.tolist()
            down_index_list.reverse() 

            train_batch_sentences_down = []
            train_batch_tags_down = []
            train_len_down = []

            for i in range(min(self.batch_size,len(train_batch_sentences))):#防止最后一个不足batch
                train_batch_sentences_down.append(train_batch_sentences[down_index_list[i]])
                train_batch_tags_down.append(train_batch_tags[down_index_list[i]])
                train_len_down.append(train_len[down_index_list[i]])

            # 已经按长度降序排列
            tensor_sentences = torch.tensor(train_batch_sentences_down, dtype = torch.long)
            # tags 长度不一致，所以不可以直接tensor
            #tensor_tags = torch.tensor(train_batch_tags_down, dtype = torch.long)

            return (tensor_sentences, train_batch_tags_down, train_len_down)

        def gene_traindata(self):
            '''
            testdata loader for testing;
            不基于batch了，不必对齐
            '''
            inputs_set = []
            targets_set = []
            for item in self.training_data:
                inputs = self.prepare_sequence(item[0],self.word_to_ix)
                targets = self.prepare_sequence(item[1],self.config.TAG_to_ix)

                inputs_set.append(inputs)
                targets_set.append(targets)

            return inputs_set, targets_set

        def gene_testdata(self):
            '''
            testdata loader for testing;
            不基于batch了，不必对齐
            '''
            inputs_set = []
            targets_set = []
            for item in self.testing_data:
                inputs = self.prepare_sequence(item[0],self.word_to_ix)
                targets = self.prepare_sequence(item[1],self.config.TAG_to_ix)

                inputs_set.append(inputs)
                targets_set.append(targets)

            return inputs_set, targets_set

    ```

2. 模型建立、训练 lstm.py
    ```python
    #coding: UTF-8
    import time

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    import numpy as np
    from sklearn.metrics import confusion_matrix
    import matplotlib.pyplot as plt
    import seaborn as sns
    from dataset import Dataset, config

    #模型定义
    class LSTMTagger(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, number_layers, drop_rate, batch_size, bidirect):
        super(LSTMTagger,self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        self.tagset_size = tagset_size
        self.num_layers = number_layers
        self.drop_rate = drop_rate
        self.batch_size = batch_size
        self.bidirect = bidirect

        self.word_embeddings = nn.Embedding(self.vocab_size,self.embedding_dim)
        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = self.hidden_dim, num_layers = self.num_layers, \
                            dropout = self.drop_rate, batch_first=True, bidirectional=self.bidirect)
        ## 如果batch_first为True，输入输出数据格式是(batch, seq_len, feature)
        ## 为False，输入输出数据格式是(seq_len, batch, feature)，
        self.dropout = nn.Dropout(self.drop_rate)

        if self.bidirect:
            print("双向LSTM")
            self.hidden2tag = nn.Linear(self.hidden_dim*2, self.tagset_size)
        else:
            print("单向LSTM")
            self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)# hidden to tag

    def forward(self, sentence, lens_batch, is_test=False):
        embeds = self.word_embeddings(sentence)# sentence 是词seq的索引

        if is_test==False:
            self.input_tensor = nn.utils.rnn.pack_padded_sequence(embeds, lens_batch, batch_first=True)
            lstm_out,self.hidden = self.lstm(self.input_tensor)
            lstm_out_pack,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)

            x = self.dropout(lstm_out_pack)#add
            x = torch.tanh(x)#add
            tag_space = self.hidden2tag(lstm_out_pack)

        else:
            lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1))
            x = self.dropout(lstm_out)
            x = torch.tanh(x)
            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))

        return tag_space

    # 训练实例
    class LSTM_Model(object):
        def __init__(self, args, lstmTagger = LSTMTagger, DataSet = Dataset, config = config):
            self.config = config
            self.args = args
            self.epoch = self.args.epoch
            self.batch_size = self.config.BATCH_SIZE

            self.dataset = DataSet(self.config)
            self.training_data = self.dataset.training_data
            self.tag_weights = self.dataset.tag_weights
            self.test_data, self.test_tags = self.dataset.gene_testdata()
            self.word_to_ix_length = self.dataset.word_to_ix_length #词表长度

            self.max_accuracy = 0.

            if args.weighted_tag: #是否对类别加权
                self.tag_weights = torch.tensor(self.tag_weights,dtype=torch.float)
                self.loss_function = nn.CrossEntropyLoss(weight=self.tag_weights)
            else:
                self.loss_function = nn.CrossEntropyLoss()

            self.lstm_model = lstmTagger(self.config.EMBEDDING_DIM, self.config.HIDDEN_DIM, self.   word_to_ix_length,\
                              len(self.config.TAG_to_ix), self.config.LAYER, self.config.DROP_RATE, self.   config.BATCH_SIZE,\
                              self.args.bidirection)

        #模型训练
        def train(self):
            self.lstm_model.train() # set mode
            self.optimizer = optim.SGD(self.lstm_model.parameters(), lr=self.config.LR,     weight_decay=self.config.weight_decay)

        self.lstm_model.cuda(0)
        self.loss_function.cuda(0)

        for epoch in range(self.epoch):
            print("\n================= Epoch:",epoch,"/",self.epoch,"===============")
            start = time.perf_counter()
            _iter,loss_total = 0,0

            while _iter < len(self.training_data):
                #try:
                    sentences_batch,tags_batch_list,lens_batch = self.dataset.gene_batch(self.training_data, _iter) #（tensor 形式的sentens,tags）
                    sentences_batch = sentences_batch.cuda(0)
                    for i in range(len(tags_batch_list)):
                        tags_batch_list[i] = torch.tensor(tags_batch_list[i], dtype=torch.long)
                        tags_batch_list[i] = tags_batch_list[i].cuda(0)

                    self.optimizer.zero_grad()
                    
                    tag_scores = self.lstm_model(sentences_batch, lens_batch)

                    # tags_batch_list = batch_size * 词个数 (对应词性的索引) # tag_scores = batch_size * 词个数 * 26类
                    loss = 0
                    for i in range(min(self.batch_size,len(tags_batch_list))):
                        loss += self.loss_function(tag_scores[i][:lens_batch[i]], tags_batch_list[i])
                    loss_total += loss

                    if (_iter + self.batch_size) % (100 * self.batch_size) == 0:
                        print("iter ",_iter, "loss:",loss_total/100, " lr:",self.optimizer.state_dict()['param_groups'][0]['lr'])
                        loss_total = 0
                        
                    _iter += self.batch_size
                    loss.backward()
                    self.optimizer.step()
                #except:
                #    print(item)
                #    continue

            # Test
            self.test()
            print("One epoch use time:",time.perf_counter()-start)

        def test(self):
            print("******Testing...")
            self.lstm_model.cuda(0)
            with torch.no_grad(): # test mode
                num = 0
                total_word = 0
                test_labels = []
                test_predicts = []
                for inputs, targets in zip(self.test_data[:3850],self.test_tags[:3850]):
                    #try:
                        total_word += len(inputs) #测试集的所有词数量
                        inputs = torch.tensor(inputs, dtype=torch.long)
                        targets = torch.tensor(targets, dtype=torch.long)

                        inputs = inputs.cuda(0)
                        targets = targets.cuda(0)

                        tag_scores = self.lstm_model(inputs,[0],is_test=True) # tensor N个词*tag数
                        tag_scores_numpy = tag_scores.cpu().numpy()

                        for idx,word in enumerate(tag_scores_numpy):
                            test_tag = np.where(word == np.max(word))
                            if test_tag[0][0] == int(targets[idx]):
                                num += 1
                        test_labels.append(int(targets[idx]))
                        test_predicts.append(test_tag[0][0])
                #except:
                #    print(item)
                #    continue

            # 评测
            accuracy = num / total_word
            if accuracy > self.max_accuracy:
                self.max_accuracy = accuracy
                # 计算混淆矩阵
                self.Confusion_matrix(test_labels, test_predicts)
                # save model
                torch.save(self.lstm_model.state_dict(),self.args.checkpoint+'/epoch_max_accuracy.pkl')
                print("Max accuracy's model is saved in ",self.args.checkpoint+'/epoch_max_accuracy.pkl')
            
            print("Acc:",accuracy,"(",num,'/',total_word,")","   Max acc so far:",self.max_accuracy) #准确率
            
    def Confusion_matrix(self, test_labels, test_predicts):
        label_list = self.dataset.TAG_list_test
        cm = confusion_matrix(test_labels, test_predicts)
        cm = cm.astype(np.float32)
        sums = []
        for i in range(len(label_list)-1):#'x'类别没有
            sums.append(np.sum(cm[i]))
        
        for i in range(len(sums)):
            for j in range(len(sums)):
                cm[i][j]=round(float(cm[i][j])/float(sums[i]),2)

        np.savetxt(self.args.checkpoint+'/Con_Matrix.txt', cm, fmt="%.2f", delimiter=',') #保存为2位小数的浮点数，用逗号分隔
        print("The confusion matrix is saved in "+self.args.checkpoint+"/Con_Matrix.txt")

 
    ```

3. 模型测试 test.py
    ```python
    #coding: UTF-8
    import os
    import argparse
    import torch
    from dataset import Dataset, config
    from lstm import LSTMTagger
    import numpy as np
    import warnings
    warnings.filterwarnings("ignore")

    parser = argparse.ArgumentParser()
    parser.add_argument('-g', '--gpu', default = [], nargs='+', type=str, help='Specify GPU id.')
    parser.add_argument( '-e', '--epoch', default = 1, type=int, help='train epoch number')
    parser.add_argument( '--checkpoint', default = 'checkpoint/', type=str, help='checkpoint.')
    parser.add_argument( '--seed', default = 1, type=int, help='seed for pytorch init')
    parser.add_argument( '-b','--bidirection', action='store_true', help='use bi-direction lstm or not')
    parser.add_argument('--weighted_tag', action='store_true',help='use wighted loss or not' )
    parser.add_argument('--crf', action='store_true',help='use crf or not' )
    args = parser.parse_args()

    if args.gpu:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu[0]

    dataset_tool = Dataset(config)


    #print("LSTM model")
    model = LSTMTagger(config.EMBEDDING_DIM, config.HIDDEN_DIM, dataset_tool.word_to_ix_length,\
                              len(config.TAG_to_ix), config.LAYER, config.DROP_RATE, 1, args.bidirection)

    if os.path.exists(args.checkpoint+"/epoch_max_accuracy.pkl"):
        model.load_state_dict(torch.load(args.checkpoint+"/epoch_max_accuracy.pkl"))
        print("Loading model...\n")
    else:
        print("未找到模型保存文件，请根据README.md中的百度云链接进行下载")
        exit()

    word_dict = {'n':'名词','t':'时间词','s':'处所词','f':'方位词','m':'数词','q':'量词','b':'区别词','r':' 代词','v':'动词','a':'形容词','z':'状态词','d':'副词','p':'介词','c':'连词','u':'助词','y':'语气词', 'e':'叹词','o':'拟声词','i':'成语','l':'习用语','j':'简称','h':'前接成分','k':'后接成分','g':'语素', 'x':'非语素字','w':'标点符号'}

    if __name__ == '__main__':
        sentence = input("请输入待标注句子(词语间用空格隔开，如“我 爱 你 中国”):")
        word_str = sentence.split()
        print("输入句子:",word_str)
        print("\nRunning...")
        inputs = dataset_tool.prepare_sequence(word_str,dataset_tool.word_to_ix)
        inputs = torch.tensor(inputs, dtype=torch.long)
        inputs = inputs.cuda(0)
        model = model.cuda(0)
        if args.crf:
            with torch.no_grad():
                score, pred_tag = model(inputs)
            result = []
            for item in pred_tag:
                result.append(list(config.TAG_to_ix.keys())[item])
        else:
            with torch.no_grad():
                result = []
                score = model(inputs,[],is_test=True)
                for word in score.cpu().numpy():
                    pred = np.where(word == np.max(word))
                    result.append(list(config.TAG_to_ix.keys())[pred[0][0]])
        print("\n词类标注结果:")
        print(result)
        for r in result:
            print(word_dict[r],end=" ")
        print()

    ```
<br> 


## 使用说明

#### 1.  实验数据准备

使用北京大学人民日报语料库，已分词的txt文档被存放在 data 路径下, 其中 1998-01-2003_shuf.txt 为经过随机shuffle的数据。
<br> 

#### 2.  LSTM网络模型训练

```python main.py --epoch 100 --checkpoint checkpoint_lstm --gpu 0 --seed 1```

参数含义：

--epoch: 训练epoch数

--checkpoint: 模型存储路径

--gpu: GPU序号

--seed: 模型初始化随机种子设置

--weighted_tag：计算损失函数时对类别加权 (可选)

默认为基于mini-batch的模型训练，若要修改batch size大小，可修改 dataset.py 中 Config 类的 BATCH_SIZE 参数。

- 训练过程，时间比较久，大概花了 140+  分钟
    ```python
    ================= Epoch: 20 / 100 ===============
    iter  2376 loss: tensor(4.0030, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  4776 loss: tensor(3.9492, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  7176 loss: tensor(3.9050, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  9576 loss: tensor(3.9904, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  11976 loss: tensor(4.0162, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  14376 loss: tensor(3.9467, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    ******Testing...
    The confusion matrix is saved in checkpoint_lstm/Con_Matrix.txt
    Max accuracy's model is saved in  checkpoint_lstm/epoch_max_accuracy.pkl
    Acc: 0.869956183669438 ( 193781 / 222748 )    Max acc so far: 0.869956183669438
    One epoch use time: 54.73565840000083

    ================= Epoch: 21 / 100 ===============
    iter  2376 loss: tensor(3.8560, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  4776 loss: tensor(3.8032, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  7176 loss: tensor(3.7554, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  9576 loss: tensor(3.8449, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  11976 loss: tensor(3.8725, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    iter  14376 loss: tensor(3.8043, device='cuda:0', grad_fn=<DivBackward0>)  lr: 0.015
    ******Testing...
    The confusion matrix is saved in checkpoint_lstm/Con_Matrix.txt
    Max accuracy's model is saved in  checkpoint_lstm/epoch_max_accuracy.pkl
    Acc: 0.8707059098173721 ( 193948 / 222748 )    Max acc so far: 0.8707059098173721
    One epoch use time: 80.64189479999914
    ```
- 模型准确率约为：88.384%

<br>

#### 3.  模型使用

加载checkpoint路径中训练好的模型，并按提示输入语句进行标注测试。

- LSTM模型

    ```python test.py --checkpoint checkpoint_lstm/ --gpu 0 ```

- 测试示例1：
    ```python
    (pytorch) C:\Users\lilin\Desktop\TagCN-LSTM>python test.py --checkpoint checkpoint_lstm/ --gpu 0
    ./data/1998-01-2003_shuf.txt
    单向LSTM
    加载模型...

    请输入句子，词语间用空格隔开，如“我 在 学习 自然 语言 处理”:我 在 学习 自然 语言 处理
    输入: ['我', '在', '学习', '自然', '语言', '处理']

    计算中...

    标注结果:
    ['r', 'p', 'v', 'n', 'n', 'v']
    代词 介词 动词 名词 名词 动词
    ```
- 测试示例2：
    ```python
    (pytorch) C:\Users\lilin\Desktop\TagCN-LSTM>python test.py --checkpoint checkpoint_lstm/ --gpu 0
    ./data/1998-01-2003_shuf.txt
    单向LSTM
    加载模型...

    请输入句子，词语间用空格隔开，如“我 在 学习 自然 语言 处理”:我 在 做 汉语 词 性 标注 的 作业
    输入: ['我', '在', '做', '汉语', '词', '性', '标注', '的', '作业']

    计算中...

    标注结果:
    ['r', 'p', 'v', 'n', 'n', 'n', 'v', 'u', 'v']
    代词 介词 动词 名词 名词 名词 动词 助词 动词

    ```
- 测试示例3：
    ```python
    (pytorch) C:\Users\lilin\Desktop\TagCN-LSTM>python test.py --checkpoint checkpoint_lstm/ --gpu 0
    ./data/1998-01-2003_shuf.txt
    单向LSTM
    加载模型...

    请输入句子，词语间用空格隔开，如“我 在 学习 自然 语言 处理”:各地 以 切实 行动 落实 总书记 对 低温 雨 雪 冰冻 灾害 防范 应对 工作 作出 的 重要 指示
    输入: ['各地', '以', '切实', '行动', '落实', '总书记', '对', '低温', '雨', '雪', '冰冻', '灾害', '防范', '应对  ', '工作', '作出', '的', '重要', '指示']

    计算中...

    标注结果:
    ['r', 'p', 'a', 'v', 'v', 'n', 'p', 'n', 'n', 'n', 'n', 'n', 'v', 'v', 'v', 'v', 'u', 'a', 'n']
    代词 介词 形容词 动词 动词 名词 介词 名词 名词 名词 名词 名词 动词 动词 动词 动词 助词 形容词 名词

    ```

* 注：名词(n)、时间词(t)、处所词(s)、方位词(f)、数词(m)、量词(q)、区别词(b)、代词( r)、动词(v)、形容词( a)、状态词(z)、副词(d)、介词( p)、连词( c)、助词(u)、语气词(y)、叹词(e)、拟声词(o)、成语(i)、习用语(l)、简称(j)、前接成分(h)、后接成分(k)、语素(g)、非语素字(x)、标点符号(w)。








